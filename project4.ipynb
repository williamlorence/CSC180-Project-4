{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec575970-aefa-472b-ad80-c053eb8c9c64",
   "metadata": {},
   "source": [
    "# Dataset Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab219583-9a83-4a42-ab9e-8bcf1457c7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./AAPL.csv\n",
      "         Date      Open      High       Low     Close  Adj Close       Volume\n",
      "0  1980-12-12  0.513393  0.515625  0.513393  0.513393   0.023186  117258400.0\n",
      "1  1980-12-15  0.488839  0.488839  0.486607  0.486607   0.021977   43971200.0\n",
      "2  1980-12-16  0.453125  0.453125  0.450893  0.450893   0.020364   26432000.0\n",
      "3  1980-12-17  0.462054  0.464286  0.462054  0.462054   0.020868   21610400.0\n",
      "4  1980-12-18  0.475446  0.477679  0.475446  0.475446   0.021473   18362400.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "#load data\n",
    "AAPL_path = \"./AAPL.csv\"\n",
    "\n",
    "df = pd.read_csv(AAPL_path)\n",
    "\n",
    "\n",
    "print(AAPL_path)\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "413f3261-9135-46cd-a382-1f1932d286a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the filtered data:\n",
      "       Open      High       Low     Close       Volume\n",
      "0  0.513393  0.515625  0.513393  0.513393  117258400.0\n",
      "1  0.488839  0.488839  0.486607  0.486607   43971200.0\n",
      "2  0.453125  0.453125  0.450893  0.450893   26432000.0\n",
      "3  0.462054  0.464286  0.462054  0.462054   21610400.0\n",
      "4  0.475446  0.477679  0.475446  0.475446   18362400.0\n"
     ]
    }
   ],
   "source": [
    "# filter out dropping Date and Adj Close \n",
    "\n",
    "df_filtered = df.drop(columns=['Date', 'Adj Close'])\n",
    "print(\"First 5 rows of the filtered data:\")\n",
    "print(df_filtered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff05719d-7149-4c9c-951c-94e381216d43",
   "metadata": {},
   "source": [
    "## Data Splitting and Preparation 70% Training data and 30 % test Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29e0ed58-f67b-42c6-a01f-dfdd1762a6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data_train shape: (6689, 5)\n",
      "X_data_test shape: (2867, 5)\n",
      "Y_data_train shape: (6689,)\n",
      "Y_data_test shape: (2867,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define input features and target variable\n",
    "X_data = df_filtered[['Open', 'High', 'Low', 'Volume', 'Close']]  # Include 'Close' as an input feature\n",
    "Y_data = df_filtered['Close']  # Target variable\n",
    "\n",
    "# Split data into 70% training and 30% testing\n",
    "X_data_train, X_data_test, Y_data_train, Y_data_test = train_test_split(\n",
    "    X_data, Y_data, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Print shapes to confirm the split\n",
    "print(\"X_data_train shape:\", X_data_train.shape)\n",
    "print(\"X_data_test shape:\", X_data_test.shape)\n",
    "print(\"Y_data_train shape:\", Y_data_train.shape)\n",
    "print(\"Y_data_test shape:\", Y_data_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf9470-655b-4551-b9d5-92c05ae1aa3b",
   "metadata": {},
   "source": [
    "## Normalizing the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6bac8f-e9d8-4fd8-b918-72878a007f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of scaled training data (first 5 rows):\n",
      "[[0.00110358 0.00120188 0.00110359 0.03047444 0.00116925]\n",
      " [0.006999   0.00695167 0.00689743 0.02363693 0.0068986 ]\n",
      " [0.00619067 0.00635554 0.00625695 0.01648095 0.0063627 ]\n",
      " [0.31317699 0.31596346 0.31704226 0.08142667 0.32020443]\n",
      " [0.47090254 0.47297309 0.47676629 0.01376061 0.479273  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on training data and transform both training and testing data\n",
    "X_data_train_scaled = scaler.fit_transform(X_data_train)\n",
    "X_data_test_scaled = scaler.transform(X_data_test)\n",
    "\n",
    "# Confirm the normalization\n",
    "print(\"Sample of scaled training data (first 5 rows):\")\n",
    "print(X_data_train_scaled[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281cac3-85f4-4829-982a-4d07ea293182",
   "metadata": {},
   "source": [
    "## Build the Fully-Connected Neural Network (FCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efe7c01c-ff49-45b1-b3e3-91826f59fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FCNN model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_data_train_scaled.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer with a single neuron for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025b020-c532-4b41-8af3-f28094573d6c",
   "metadata": {},
   "source": [
    "## Set Up Callbacks and Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1656c862-f8d6-4b20-994a-a29891c5bbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 2/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 3/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 4/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 5/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 6/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 7/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 8/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 9/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 10/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "FCNN model training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Set up callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_fcnn_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_data_train_scaled, Y_data_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Print training completion\n",
    "print(\"FCNN model training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12430480-b3f6-482f-a8e7-02f6b99350e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in Y_data_test: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of NaNs in Y_data_test:\", Y_data_test.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2485515-2eba-4f1f-8906-30f2a763c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in predictions: 2867\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of NaNs in predictions:\", np.isnan(predictions).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0490125b-7fbd-4489-bc27-cf15e9a8c107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in X_data_train_scaled: 5\n",
      "Number of NaNs in X_data_test_scaled: 0\n",
      "Number of Infs in X_data_train_scaled: 0\n",
      "Number of Infs in X_data_test_scaled: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of NaNs in X_data_train_scaled:\", np.isnan(X_data_train_scaled).sum())\n",
    "print(\"Number of NaNs in X_data_test_scaled:\", np.isnan(X_data_test_scaled).sum())\n",
    "print(\"Number of Infs in X_data_train_scaled:\", np.isinf(X_data_train_scaled).sum())\n",
    "print(\"Number of Infs in X_data_test_scaled:\", np.isinf(X_data_test_scaled).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79204561-3fee-4f5d-95cd-e3d8c74fd9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NaN values in X_data_train:\n",
      "     Open  High  Low  Volume  Close\n",
      "165   NaN   NaN  NaN     NaN    NaN\n"
     ]
    }
   ],
   "source": [
    "nan_rows = X_data_train[X_data_train.isna().any(axis=1)]\n",
    "print(\"Rows with NaN values in X_data_train:\")\n",
    "print(nan_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db846a4f-d484-4259-93c7-37951a389c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in X_data_train after removal: 0\n",
      "Number of NaNs in Y_data_train after removal: 0\n"
     ]
    }
   ],
   "source": [
    "# Get the index of rows with NaN values in X_data_train\n",
    "nan_indices = X_data_train.index[X_data_train.isna().any(axis=1)]\n",
    "\n",
    "# Drop the rows with NaN values from both X_data_train and Y_data_train\n",
    "X_data_train = X_data_train.drop(index=nan_indices)\n",
    "Y_data_train = Y_data_train.drop(index=nan_indices)\n",
    "\n",
    "# Confirm the removal\n",
    "print(\"Number of NaNs in X_data_train after removal:\", X_data_train.isna().sum().sum())\n",
    "print(\"Number of NaNs in Y_data_train after removal:\", Y_data_train.isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2752376-b3b3-46ab-92e1-24f5fbfed5d4",
   "metadata": {},
   "source": [
    "## Re-normalize X_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "977018cb-4f54-46ce-b53e-fd5a2ce2e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of scaled training data (first 5 rows):\n",
      "[[0.00110358 0.00120188 0.00110359 0.03047444 0.00116925]\n",
      " [0.006999   0.00695167 0.00689743 0.02363693 0.0068986 ]\n",
      " [0.00619067 0.00635554 0.00625695 0.01648095 0.0063627 ]\n",
      " [0.31317699 0.31596346 0.31704226 0.08142667 0.32020443]\n",
      " [0.47090254 0.47297309 0.47676629 0.01376061 0.479273  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Re-fit and transform the cleaned X_data_train\n",
    "X_data_train_scaled = scaler.fit_transform(X_data_train)\n",
    "\n",
    "# Transform X_data_test using the fitted scaler\n",
    "X_data_test_scaled = scaler.transform(X_data_test)\n",
    "\n",
    "# Confirm normalization\n",
    "print(\"Sample of scaled training data (first 5 rows):\")\n",
    "print(X_data_train_scaled[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b93bf-8c0b-4dda-b336-32de08661cdc",
   "metadata": {},
   "source": [
    "## Re-training the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd6a83de-875c-46b1-b114-a84cb4c3884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 2/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 3/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 4/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 5/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 6/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 7/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 8/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 9/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 10/100\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step\n",
      "Number of NaNs in predictions after re-training: 2867\n"
     ]
    }
   ],
   "source": [
    "# Re-train the model using the cleaned data\n",
    "history = model.fit(\n",
    "    X_data_train_scaled, Y_data_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Generate predictions on the test set\n",
    "predictions = model.predict(X_data_test_scaled)\n",
    "\n",
    "# Check for NaNs in predictions\n",
    "print(\"Number of NaNs in predictions after re-training:\", np.isnan(predictions).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f977060-e021-4e79-b451-e964fc114dfd",
   "metadata": {},
   "source": [
    "## Evaluate the Model and Plot the Regression Lift Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ce4c286-15b6-42e0-91ba-2e8b57afc221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in predictions: 2867\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of NaNs in predictions:\", np.isnan(predictions).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4dce461d-f71e-46b4-a0fb-551b776d9259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error', metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddfd5c82-29f7-4e6a-9723-a18241464639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tf_env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Rebuild the model with dropout layers for regularization\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_data_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),  # Dropout to prevent overfitting\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# Recompile the model after adding dropout\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error', metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2408563e-f9fe-479a-9d33-d61d54907be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and max of X_data_train_scaled: 0.0 1.0\n",
      "Min and max of X_data_test_scaled: 3.845767079230136e-05 1.013463801591929\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Min and max of X_data_train_scaled:\", X_data_train_scaled.min(), X_data_train_scaled.max())\n",
    "print(\"Min and max of X_data_test_scaled:\", X_data_test_scaled.min(), X_data_test_scaled.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca21ab28-26c0-46af-9eba-268930459dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "# Modify the model to include a different weight initializer\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', kernel_initializer=HeNormal(), input_shape=(X_data_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu', kernel_initializer=HeNormal()),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78d4dab3-3c29-4728-bb1a-953a19f5f8b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate RMSE\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_data_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest RMSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rmse)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Plot the regression lift chart\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:506\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    503\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    504\u001b[0m         )\n\u001b[1;32m--> 506\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    510\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:113\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m    111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m--> 113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    116\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(y_true, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(Y_data_test, predictions))\n",
    "print(\"Test RMSE:\", rmse)\n",
    "\n",
    "# Plot the regression lift chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Y_data_test, predictions, alpha=0.6)\n",
    "plt.plot([min(Y_data_test), max(Y_data_test)], [min(Y_data_test), max(Y_data_test)], color='red')\n",
    "plt.xlabel('Actual Close Prices')\n",
    "plt.ylabel('Predicted Close Prices')\n",
    "plt.title('Regression Lift Chart')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5581bd3-596c-433c-bf91-c62ca92428c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
